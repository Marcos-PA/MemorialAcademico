# On Evaluating the Efficiency of Source Code Generated by LLMs

Niu, Changan; Zhang, Ting; Li, Chuanyi; Luo, Bin; Ng, Vincent. "On Evaluating the Efficiency of Source Code Generated by LLMs," in AI Foundation Models and Software Engineering (FORGE ’24), April 14, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 5 pages. doi:
[10.1145/3650105.3652295](https://doi.org/10.1145/3650105.3652295)

## 1. Fichamento de Conteúdo
O artigo investiga a eficiência do código gerado por Modelos de Linguagem de Grande Escala (LLMs). Os autores avaliam a execução do código gerado em benchmarks como HumanEval, MBPP e um conjunto de problemas do LeetCode. O estudo busca responder duas perguntas principais:

1. Quão eficiente é o código gerado por LLMs?
2. Como melhorar a eficiência do código gerado por LLMs via *prompting*?

Para responder à primeira questão, os autores comparam o tempo de execução dos códigos gerados por diferentes modelos (GPT-4, GPT-3.5, Code Llama, WizardCoder, DeepSeek Coder, entre outros). Os resultados mostram que a correlação entre a taxa de sucesso na geração de código correto e sua eficiência não é direta. Por exemplo, um modelo pode produzir mais soluções corretas, mas com menor eficiência computacional.

Na segunda parte do estudo, os pesquisadores exploram três técnicas de *prompting* para melhorar a eficiência do código gerado. As abordagens incluem:
- Pedido direto para otimização.
- Pedido para otimização após a geração inicial do código.
- Pedido para uma estratégia de otimização antes da geração otimizada.

Os resultados mostram que a abordagem em etapas (“Chain-of-Thought”) melhora significativamente a eficiência, especialmente em problemas mais complexos.

## 2. Fichamento Bibliográfico

* **Modelos Avaliados**: Foram testados GPT-4, GPT-3.5, Phi-2, Code Llama (7B, 13B, 34B), WizardCoder (7B, 13B, 34B) e DeepSeek Coder (33B Base e Instruct) (página 104).
* **Chain-of-Thought (CoT)** prompting: Induz o LLM a explicar seu raciocínio passo a passo, resultando em previsões mais interpretáveis.
* **Eficiência dos Modelos**: Em benchmarks como HumanEval e MBPP, DeepSeek Coder 33B Instruct teve a menor latência, enquanto GPT-4 apresentou maior eficiência em LeetCodeEval (página 105).
* **Impacto do *Prompting***: As técnicas de *prompting* melhoraram a eficiência em até 18% no LeetCodeEval (página 106).
* **Complexidade Computacional**: A eficiência do código varia conforme o problema, com modelos treinados para otimização gerando soluções melhores (página 106).

### Tabela Comparativa de Eficiência

| Modelo | HumanEval Runtime | MBPP Runtime | LeetCodeEval Easy | LeetCodeEval Medium |
|--------|-------------------|--------------|-------------------|---------------------|
| GPT-4  | 8.61              | 9.14         | 30.89             | 50.92               |
| GPT-3.5 | 8.35              | 8.86         | 33.80             | -                   |
| DeepSeek Coder 33B | 7.54 | 8.93 | 35.30 | 49.08 |



## 2. Fichamento Bibliográfico

* **Avaliação de eficiência**: A pesquisa propõe que a avaliação de código gerado por LLMs deve considerar não apenas sua correção, mas também sua eficiência em tempo de execução (página 103).
* **Benchmarks utilizados**: O estudo utiliza os conjuntos HumanEval, MBPP e LeetCodeEval para mensurar a eficiência do código gerado pelos modelos (página 104).
* **Influência do _prompting_**: Diferentes abordagens de _prompting_ foram testadas para gerar código mais eficiente, sendo que abordagens em múltiplas etapas produziram melhores resultados (página 105).
* **Resultados experimentais**: Os modelos GPT-4 e DeepSeek Coder mostraram desempenho superior na geração de código eficiente, mas o tamanho do modelo não foi um fator determinante (página 106).
* **Impacto na produtividade**: Melhorar a eficiência do código gerado pode reduzir o tempo de desenvolvimento e otimizar a aceitação dos códigos por desenvolvedores (página 106).

## 3. Fichamento de Citações

* _"More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming."_
* _"The ability to generate correct code is not positively correlated with the ability to generate efficient code."_
* _"Step-by-step prompting could make LLMs generate more efficient code, especially on complex problems."_
* _"We propose a LeetCode-based benchmark which provides a reference point for comparing the correctness and efficiency of more complex code."_
* _"Training strategy and data have an impact on the efficiency of the generated code."_




