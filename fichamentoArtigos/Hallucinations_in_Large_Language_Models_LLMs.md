# Hallucinations in Large Language Models (LLMs)

Reddy, G. Pradeep; Kumar, Y. V. Pavan; Prakash, K. Purna. "Hallucinations in Large Language Models (LLMs)," in IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream), 2024. 
IEEE. doi: [10.1109/ESTREAM61684.2024.10542617](https://doi.org/10.1109/ESTREAM61684.2024.10542617)

## 1. Fichamento de Conteúdo

O artigo investiga o fenômeno das alucinações em Modelos de Linguagem de Grande Escala (LLMs), que ocorre quando os modelos geram informações incorretas ou sem fundamento na realidade.
Os autores analisam as principais causas dessas alucinações, incluindo viés nos dados de treinamento, contexto limitado, _prompts_ ambíguos e ataques adversariais. O impacto das alucinações pode ser severo em aplicações críticas,
como saúde e direito, onde informações imprecisas podem levar a decisões equivocadas. Para mitigar esse problema, são sugeridas abordagens como curadoria de dados, melhorias na representação de contexto, participação humana no processo de validação (_human-in-the-loop_) e implementação de mecanismos de filtragem e verificação factual.
O estudo enfatiza a importância de estratégias robustas para garantir a confiabilidade do conteúdo gerado por LLMs.

## 2. Fichamento Bibliográfico

* **Definição de alucinações em LLMs**: Ocorrem quando os modelos geram respostas que parecem plausíveis, mas são incorretas ou sem suporte factual (página 2).
* **Principais causas das alucinações**: Incluem viés nos dados de treinamento, falta de contexto adequado, ambiguidade nos _prompts_ e sobreajuste do modelo (página 4).
* **Impacto em aplicações críticas**: Áreas como saúde e direito são especialmente vulneráveis a erros causados por alucinações de IA (página 6).
* **Estratégias de mitigação**: Curadoria de dados, aprimoramento da representação de contexto e filtros para verificação de informações (página 7).
* **Importância do _human-in-the-loop_**: A intervenção humana na validação de saídas de IA pode reduzir significativamente o risco de alucinações (página 8).

## 3. Fichamento de Citações

* _"Hallucinations in LLMs happen when the model generates information that may sound believable but is actually wrong."_
* _"The effects of hallucinations in LLMs can be particularly severe in domains such as healthcare and law, where misinformation can lead to harmful consequences."_
* _"Dataset curation and improving context representation are essential to minimizing hallucinations in LLMs."_
* _"Human-in-the-loop validation is a crucial approach to ensuring the reliability of AI-generated content."_
* _"Filtering mechanisms can help detect and correct hallucinations, ensuring more accurate and trustworthy AI outputs."_
